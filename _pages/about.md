---
layout: about
title: Rishabh Sabharwal
permalink: /
subtitle: MSc Artificial Intelligence · University of Edinburgh

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false
  more_info: >
    <div style="text-align:center;">
      <p>London, UK</p>
    </div>

# Hide this page from the top navbar to avoid "about about"
# (this page is still your homepage at /)
nav: false
nav_order: 1

selected_papers: true
social: true

announcements:
  enabled: true
  scrollable: true
  limit: 5

latest_posts:
  enabled: false
  scrollable: false
  limit: 3
---

I am a master's student at the University of Edinburgh, being supervised by <a href="https://scholar.google.com/citations?user=zLDAY8QAAAAJ&hl=en" target="_blank" rel="noopener">Prof. Jeff Pan</a>. Previously, I was a predoctoral researcher at the Indian Institute of Science (Bengaluru), where I worked with <a href="https://www.vistalabiisc.com/P_portfolio" target="_blank" rel="noopener">Prof. Punit Rathore</a> on projects related to graph neural networks and computer vision. I also worked with <a href="https://scholar.google.com/citations?user=Gu8FjdwAAAAJ&hl=en" target="_blank" rel="noopener">Prof. Sundeep Chepuri</a> on one of these projects.

My current interests include mechanistic interpretability in LLMs and AI safety. I’m interested in understanding how models behave under distribution shift and adversarial conditions, such as when inputs are out-of-domain or intentionally designed to challenge the system. These situations often reveal capabilities and failure modes that do not show up in standard evaluations, helping us understand how reliably these models operate in real-world applications when deployed to assist people.

For my master’s thesis, I’m developing a benchmark to assess **self-evolving LLM agents** that integrate web search and coding tools within a unified workflow. The benchmark aims to measure the synergy of these capabilities by testing end-to-end tasks in which an agent must search for information, validate and interpret it, and then write robust code to automate subtasks or create reusable components for longer pipelines. Agents will be evaluated based on final correctness, adaptability, iterative planning, and tool development.

### Research keywords
Mechanistic interpretability · AI safety · LLM agents · Distribution shift · Adversarial robustness · Graph neural networks
